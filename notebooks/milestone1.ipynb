{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-berlin",
   "metadata": {},
   "source": [
    "# Milestone 1 - Tackling big data on a laptop\n",
    "\n",
    "#### Yuanzhe Marco Ma, Matthew Pin, Mark Wang, Tingyu Zhang\n",
    "\n",
    "* [Download data](#1.-Download-the-data)\n",
    "* [Combine data](#2.-Combine-the-data)\n",
    "* [EDA with Python](#3.-EDA-with-python)\n",
    "* [EDA with R](#4.-EDA-with-R)\n",
    "* [Reflection](#5.-Reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swiss-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forty-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-article",
   "metadata": {},
   "source": [
    "# 1. Download the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painful-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\" # This notebook should be ran mannually\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "\n",
    "for file in files:\n",
    "    if file[\"name\"] == \"data.zip\":\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-march",
   "metadata": {},
   "source": [
    "# 2. Combine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-wages",
   "metadata": {},
   "source": [
    "A peek of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pediatric-design",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 2 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 time  lat_min  lat_max  lon_min  lon_max rain (mm/day)\n",
       "npartitions=2                                                          \n",
       "               object  float64  float64  float64  float64       float64\n",
       "                  ...      ...      ...      ...      ...           ...\n",
       "                  ...      ...      ...      ...      ...           ...\n",
       "Dask Name: read-csv, 2 tasks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "see = dd.read_csv(\n",
    "    \"../data/ACCESS-CM2_daily_rainfall_NSW.csv\",\n",
    "    assume_missing=True,\n",
    ")\n",
    "\n",
    "see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "geological-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 167.07 MiB, increment: 4.51 MiB\n",
      "CPU times: user 717 ms, sys: 45.1 ms, total: 762 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "combined_data = dd.from_pandas(pd.DataFrame({\"time\": [], \"lat_min\": [], \n",
    "                                             \"lat_max\": [], \"lon_min\": [], \n",
    "                                             \"lon_max\": [], \"rain (mm/day)\": [], \n",
    "                                             \"model\": []}), npartitions=1)\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename[-4: ] == \".csv\":\n",
    "        model = filename.partition('_daily_rainfall')[0]\n",
    "        ddf = dd.read_csv(output_directory + filename, assume_missing=True)\n",
    "        if len(ddf.columns) == 2:\n",
    "            ddf['lat_min'] = None\n",
    "            ddf['lat_max'] = None\n",
    "            ddf['lon_min'] = None\n",
    "            ddf['lon_max'] = None\n",
    "            ddf = ddf[['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']]\n",
    "        ddf[\"model\"] = model\n",
    "        combined_data = dd.concat([combined_data, ddf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stretch-boulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1882.38 MiB, increment: 1715.30 MiB\n",
      "CPU times: user 7min 30s, sys: 37.3 s, total: 8min 7s\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "combined_data.to_csv(output_directory + \"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-costa",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**Compare run times and memory usages by using DASK on different machines within our team:**\n",
    "\n",
    "- Macbook Air 13 inch 2019, 8GB RAM, 1.6 GHz Dual-Core Intel Core i5 \n",
    "    - peak memory: 1122.01 MiB, increment: 1010.62 MiB, CPU times: user 12min 19s, sys: 55.2 s, total: 13min 14s, Wall time: 12min 14s\n",
    "- Linux Desktop, 7.1G RAM, AMD Ryzen 5 5 46000h with radeon graphics x 12\n",
    "    - peak memory: 3246.38 MiB, increment: 3074.11 MiB, CPU times: user 13min 40s, sys: 1min 6s, total: 14min 47s, Wall time: 12min 48s\n",
    "- Macbook Mini 2018, 8GB RAM, 3.6 GHz-Quad-Core Intel Core i3\n",
    "    - peak memory: 986.83 MiB, increment: 816.16 MiB, CPU times: user 8min 22s, sys: 31 s, total: 8min 53s, Wall time: 8min 2s\n",
    "- Macbook Pro 2020 16GB RAM, 1.4 GHz Quad-Core Intel Core i5\n",
    "    - peak memory: 1882.38 MiB, increment: 1715.30 MiB, CPU times: user 7min 30s, sys:  37.3 s, total: 8min 7s, Wall time: 7min 14s\n",
    "\n",
    "\n",
    "In general, when we run the task of combining the data, the computer's random access memory (RAM) is heavily utilized. As you can see above, our computers took more than 8 minutes to run, and peak memories were all around 1000 MiB, but one of our team members' peak memories was around 3000 MiB (He uses Linux; everyone else in our team uses MAC System). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-forest",
   "metadata": {},
   "source": [
    "# 3. EDA with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-teaching",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "saved-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 164.46 MiB, increment: 22.59 MiB\n",
      "1.11 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "combined_data = dd.read_csv(\"../data/combined_data.csv/*\")\n",
    "combined_data = combined_data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "differential-celebrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)',\n",
      "       'model', 'Unnamed: 0.1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-radiation",
   "metadata": {},
   "source": [
    "### Method 1: Using Dask only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-crazy",
   "metadata": {},
   "source": [
    "Find geographical range of all records and mean rainfall level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "molecular-latvia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lat_min is -36.4674\n",
      "Maximum lat_max is -27.9061\n",
      "Minimum lon_min is 140.6250\n",
      "Maximum lon_max is 155.6250\n",
      "Mean rainfall is 1.9018\n",
      "peak memory: 1413.84 MiB, increment: 1257.12 MiB\n",
      "6min 34s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "print(f\"Minimum lat_min is %0.4f\" % \n",
    "      combined_data['lat_min'].astype('float64').min().compute())\n",
    "print(f\"Maximum lat_max is %0.4f\" % \n",
    "      combined_data['lat_max'].astype('float64').max().compute())\n",
    "print(f\"Minimum lon_min is %0.4f\" % \n",
    "      combined_data['lon_min'].astype('float64').min().compute())\n",
    "print(f\"Maximum lon_max is %0.4f\" % \n",
    "      combined_data['lon_max'].astype('float64').max().compute())\n",
    "print(f\"Mean rainfall is %0.4f\" % \n",
    "      combined_data['rain (mm/day)'].astype('float64').mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-motorcycle",
   "metadata": {},
   "source": [
    "### Method 2: Using Dask and change datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-pharmacy",
   "metadata": {},
   "source": [
    "#### Default datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eight-failing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time             object\n",
      "lat_min          object\n",
      "lat_max          object\n",
      "lon_min          object\n",
      "lon_max          object\n",
      "rain (mm/day)    object\n",
      "model            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-requirement",
   "metadata": {},
   "source": [
    "#### Changing to `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "characteristic-texture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with 'object' dtype:\n",
      "Index                22016\n",
      "lat_max          500110904\n",
      "lat_min          500110904\n",
      "lon_max          500110904\n",
      "lon_min          500110904\n",
      "rain (mm/day)    500110904\n",
      "dtype: int64\n",
      "\n",
      "Memory usage after changing to float32:\n",
      "Index                22016\n",
      "lat_max          250055452\n",
      "lat_min          250055452\n",
      "lon_max          250055452\n",
      "lon_min          250055452\n",
      "rain (mm/day)    250055452\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = ['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "\n",
    "print(\"Memory usage with 'object' dtype:\")\n",
    "print(combined_data[numeric_columns].memory_usage().compute())\n",
    "\n",
    "# Change dtype\n",
    "combined_data[numeric_columns] = combined_data[numeric_columns].astype('float32')\n",
    "    \n",
    "print(\"\\nMemory usage after changing to float32:\")\n",
    "print(combined_data[numeric_columns].memory_usage().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-spring",
   "metadata": {},
   "source": [
    "#### Perform the same EDA with `float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-serum",
   "metadata": {},
   "source": [
    "Find geographical range of all records and mean rainfall level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sacred-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lat_min is -36.4674\n",
      "Maximum lat_max is -27.9061\n",
      "Minimum lon_min is 140.6250\n",
      "Maximum lon_max is 155.6250\n",
      "Mean rainfall is 1.9018\n",
      "peak memory: 1533.98 MiB, increment: 1347.66 MiB\n",
      "8min 37s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "print(f\"Minimum lat_min is %0.4f\" % \n",
    "      combined_data['lat_min'].min().compute())\n",
    "print(f\"Maximum lat_max is %0.4f\" % \n",
    "      combined_data['lat_max'].max().compute())\n",
    "print(f\"Minimum lon_min is %0.4f\" % \n",
    "      combined_data['lon_min'].min().compute())\n",
    "print(f\"Maximum lon_max is %0.4f\" % \n",
    "      combined_data['lon_max'].max().compute())\n",
    "print(f\"Mean rainfall is %0.4f\" % \n",
    "      combined_data['rain (mm/day)'].mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-glass",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Loading:\n",
    "- Since the `combined_data` was stored into partitions with Dask, we had to load them with Dask also. Dask takes care of loading in chunks and in parallel. \n",
    "\n",
    "EDA with Dask:\n",
    "- Using Dask to perform the EDA, the peak memory usage was around 1413MB, and it took around 6.5 minutes to process all 5 columns. \n",
    "\n",
    "EDA with Dask and change dtype:\n",
    "- Originally, Dask saved all columns as `object` datatype. After changing the numeric columns to `float32`, the memory usage went from around 500MB per column to around 250MB per column. <br> \n",
    "- Performing the same EDA yields the same result. The peak memory usage is around the same level ~1500MB. This is because `%memit` measures the memory usage of the operation, instead of the data size. So although the data size decreased, the memory stack occupied by this code cell didn't change much. \n",
    "The slight increase was probably due to other programs in the background affecting the memory I/O. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-incident",
   "metadata": {},
   "source": [
    "# 4. EDA with R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-prediction",
   "metadata": {},
   "source": [
    "## Creating feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informal-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 8807.39 MiB, increment: 8536.95 MiB\n",
      "CPU times: user 2min 37s, sys: 42.4 s, total: 3min 20s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "combined_data.compute().reset_index().to_feather('../data/combined_data_feather.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "buried-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5G\t../data/combined_data.csv\n",
      "1.2G\t../data/combined_data_feather.feather\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/combined_data_feather.feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-swimming",
   "metadata": {},
   "source": [
    "- We can see the file size is much smaller in a feather file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-clearing",
   "metadata": {},
   "source": [
    "## Setting up R environment + importing feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formal-closer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "every-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "helpful-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "\u001b[90m# A tibble: 92,040 x 2\u001b[39m\n",
      "   time                    n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m               \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m 1889-01-01             29\n",
      "\u001b[90m 2\u001b[39m 1889-01-01 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 3\u001b[39m 1889-01-02             29\n",
      "\u001b[90m 4\u001b[39m 1889-01-02 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 5\u001b[39m 1889-01-03             29\n",
      "\u001b[90m 6\u001b[39m 1889-01-03 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 7\u001b[39m 1889-01-04             29\n",
      "\u001b[90m 8\u001b[39m 1889-01-04 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 9\u001b[39m 1889-01-05             29\n",
      "\u001b[90m10\u001b[39m 1889-01-05 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m# … with 92,030 more rows\u001b[39m\n",
      "Time difference of 19.82407 secs\n",
      "CPU times: user 18.7 s, sys: 8.44 s, total: 27.1 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "\n",
    "combined_data_r <- arrow::read_feather(\"../data/combined_data_feather.feather\")\n",
    "print(class(combined_data_r))\n",
    "result <- combined_data_r %>% count(time)\n",
    "\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-worst",
   "metadata": {},
   "source": [
    "## perform the EDA with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "distributed-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of rows:62513863\"\n",
      "[1] \"Number of cols:8\"\n",
      "[1] \"Column names are...\"\n",
      "[1] \"index\"         \"time\"          \"lat_min\"       \"lat_max\"      \n",
      "[5] \"lon_min\"       \"lon_max\"       \"rain (mm/day)\" \"model\"        \n",
      "CPU times: user 13 ms, sys: 6.06 ms, total: 19.1 ms\n",
      "Wall time: 16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "print(paste0(\"Number of rows:\", nrow(combined_data_r)))\n",
    "print(paste0(\"Number of cols:\", ncol(combined_data_r)))\n",
    "print(\"Column names are...\")\n",
    "print(paste0(colnames(combined_data_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "blessed-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "CPU times: user 11.4 ms, sys: 4.79 ms, total: 16.2 ms\n",
      "Wall time: 12.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "class(combined_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mathematical-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 8\u001b[39m\n",
      "  index time          lat_min lat_max lon_min lon_max `rain (mm/day)` model     \n",
      "  \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m     \n",
      "\u001b[90m1\u001b[39m     0 1889-01-01 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.24\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m2\u001b[39m     1 1889-01-02 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.22\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m3\u001b[39m     2 1889-01-03 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.50\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m4\u001b[39m     3 1889-01-04 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.25\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m5\u001b[39m     4 1889-01-05 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.27\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m6\u001b[39m     5 1889-01-06 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.20\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "CPU times: user 34.5 ms, sys: 5.81 ms, total: 40.3 ms\n",
      "Wall time: 36.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "head(combined_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stainless-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 8\u001b[39m\n",
      "   index time           lat_min lat_max lon_min lon_max `rain (mm/day)` model   \n",
      "   \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \n",
      "\u001b[90m1\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m958 2014-12-26 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            4.44 SAM0-UN…\n",
      "\u001b[90m2\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m959 2014-12-27 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            6.69 SAM0-UN…\n",
      "\u001b[90m3\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m960 2014-12-28 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            7.86 SAM0-UN…\n",
      "\u001b[90m4\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m961 2014-12-29 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.           10.0  SAM0-UN…\n",
      "\u001b[90m5\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m962 2014-12-30 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            8.54 SAM0-UN…\n",
      "\u001b[90m6\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m963 2014-12-31 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.           68.1  SAM0-UN…\n",
      "CPU times: user 33.1 ms, sys: 6.43 ms, total: 39.5 ms\n",
      "Wall time: 36.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "tail(combined_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "finished-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"5 Number Summary lat_min:\"\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      " -36.47  -34.87  -33.00  -33.10  -31.40  -29.90 \n",
      "CPU times: user 2.31 s, sys: 873 ms, total: 3.18 s\n",
      "Wall time: 3.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(\"5 Number Summary lat_min:\")\n",
    "summary(na.omit(combined_data_r$lat_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unlimited-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"5 Number Summary lat_max:\"\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      " -36.00  -33.66  -32.04  -31.98  -30.16  -27.91 \n",
      "CPU times: user 2.87 s, sys: 1.11 s, total: 3.98 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(\"5 Number Summary lat_max:\")\n",
    "summary(na.omit(combined_data_r$lat_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acquired-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"5 Number Summary lon_min:\"\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "  140.6   143.4   146.9   146.9   150.2   153.8 \n",
      "CPU times: user 2.39 s, sys: 1.24 s, total: 3.63 s\n",
      "Wall time: 3.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(\"5 Number Summary lon_min:\")\n",
    "summary(na.omit(combined_data_r$lon_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "crazy-extent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"5 Number Summary lon_max:\"\n",
      "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
      "  141.2   145.0   148.1   148.2   151.3   155.6 \n",
      "CPU times: user 2.77 s, sys: 1.33 s, total: 4.1 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(\"5 Number Summary lon_max:\")\n",
    "summary(na.omit(combined_data_r$lon_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "normal-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"5 Number Summary Rainfall (mm/day):\"\n",
      "    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n",
      "  0.0000   0.0000   0.0616   1.9018   1.0213 432.9395 \n",
      "CPU times: user 3.18 s, sys: 1.54 s, total: 4.72 s\n",
      "Wall time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(\"5 Number Summary Rainfall (mm/day):\")\n",
    "summary(na.omit(combined_data_r$'rain (mm/day)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-count",
   "metadata": {},
   "source": [
    "## Why we used a .feather file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-worry",
   "metadata": {},
   "source": [
    "A .feather file was used mainly due to its lightweight/portable nature.\n",
    "- The format was quick for saving the data (less than 5 minutes to save the entire dataset) and loading the data (less than 24 seconds).\n",
    "- The format was to push a very large dataframe into a significantly smaller file size than what was seen in our CSV file (1.4G vs 6.5G). \n",
    "- The format is language agnostic so we will be able to read the dataframe into either python.\n",
    "- The format was something that we were familiar with from previous projects.\n",
    "\n",
    "We chose feather instead of Parquet because...\n",
    "- Technically a Parquet file could have compressed the data further but it would have been more computationally expensive and thus slower. 1.4G is a very reasonable size for a dataset this big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-desire",
   "metadata": {},
   "source": [
    "# 5. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-royalty",
   "metadata": {},
   "source": [
    "- Challenges with downloading the data...\n",
    "    - Downloading data from flagshare took very long time due to internet connection reasons (I am using a VPN). Therefore, I let the program run overnight to obtain all the files.\n",
    "\n",
    "\n",
    "- Challenges with combining the CSVs into one file...\n",
    "    - (1) We are only able to use Dask  to read and combine the downloaded .csv files. Whenever pandas  is used, the notebook crashes. Therefore, we only choose Dask to handle the data in this milestone.\n",
    "    - (2) The format of the saved csv  files is confusing. Although I used the to_csv method, it is saved in a folder with \".csv\" in its name. There are multiple .part files in the folder. We spent some time to figure out that we need to read from the individual files in the \".csv\" folder, instead of the folder per se.\n",
    "    - (3) In order to see the first few observations in the combined dataset, I used the .head method. However, because datasets are stored in partitions in Dask , and by default there the head() method only obtains data from the first partition, there was initially not enough data to show. We specified npartitions=10 to have enough data. \n",
    "\n",
    "- Challenges with python EDA include...\n",
    "    - Performing EDA with Dask means that we cannot read only the columns we want. So computing statistics on a specific column is still slow.\n",
    "\n",
    "- Challenges with the R EDA included...\n",
    "    - Converting the combined data Dask dataframe to the exact format needed to save it as a feather file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
