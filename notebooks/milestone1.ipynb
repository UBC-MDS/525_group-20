{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noble-dictionary",
   "metadata": {},
   "source": [
    "# Milestone 1 - Tackling big data on a laptop\n",
    "\n",
    "#### Yuanzhe Marco Ma, Matthew Pin, Mark Wang, Tingyu Zhang\n",
    "\n",
    "Table of Content: \n",
    "* [Download data](#1.-Download-the-data)\n",
    "* [Combine data](#2.-Combine-the-data)\n",
    "* [EDA with Python](#3.-EDA-with-python)\n",
    "* [EDA with R](#4.-EDA-with-R)\n",
    "* [Reflection](#5.-Reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "north-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "surgical-purple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-chorus",
   "metadata": {},
   "source": [
    "# 1. Download the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "discrete-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\" # This notebook should be ran mannually\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "\n",
    "for file in files:\n",
    "    if file[\"name\"] == \"data.zip\":\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-death",
   "metadata": {},
   "source": [
    "# 2. Combine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-newfoundland",
   "metadata": {},
   "source": [
    "A peek of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "significant-blues",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-csv, 2 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 time  lat_min  lat_max  lon_min  lon_max rain (mm/day)\n",
       "npartitions=2                                                          \n",
       "               object  float64  float64  float64  float64       float64\n",
       "                  ...      ...      ...      ...      ...           ...\n",
       "                  ...      ...      ...      ...      ...           ...\n",
       "Dask Name: read-csv, 2 tasks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "see = dd.read_csv(\n",
    "    \"../data/ACCESS-CM2_daily_rainfall_NSW.csv\",\n",
    "    assume_missing=True,\n",
    ")\n",
    "\n",
    "see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cooked-jason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 113.21 MiB, increment: 9.02 MiB\n",
      "CPU times: user 980 ms, sys: 102 ms, total: 1.08 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combined_data = dd.from_pandas(pd.DataFrame({\"time\": [], \"lat_min\": [], \n",
    "                                             \"lat_max\": [], \"lon_min\": [], \n",
    "                                             \"lon_max\": [], \"rain (mm/day)\": [], \n",
    "                                             \"model\": []}), npartitions=1)\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename[-4: ] == \".csv\":\n",
    "        model = filename.partition('_daily_rainfall')[0]\n",
    "        ddf = dd.read_csv(output_directory + filename, assume_missing=True)\n",
    "        if len(ddf.columns) == 2:\n",
    "            ddf['lat_min'] = None\n",
    "            ddf['lat_max'] = None\n",
    "            ddf['lon_min'] = None\n",
    "            ddf['lon_max'] = None\n",
    "            ddf = ddf[['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']]\n",
    "        ddf[\"model\"] = model\n",
    "        combined_data = dd.concat([combined_data, ddf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rolled-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1122.01 MiB, increment: 1010.62 MiB\n",
      "CPU times: user 12min 19s, sys: 55.2 s, total: 13min 14s\n",
      "Wall time: 12min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combined_data.to_csv(output_directory + \"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-verification",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**Compare run times and memory usages by using DASK on different machines within our team:**\n",
    "\n",
    "- peak memory: 1122.01 MiB, increment: 1010.62 MiB, CPU times: user 12min 19s, sys: 55.2 s, total: 13min 14s, Wall time: 12min 14s\n",
    "- peak memory: 3246.38 MiB, increment: 3074.11 MiB, CPU times: user 13min 40s, sys: 1min 6s, total: 14min 47s, Wall time: 12min 48s\n",
    "- peak memory: 986.83 MiB, increment: 816.16 MiB, CPU times: user 8min 22s, sys: 31 s, total: 8min 53s, Wall time: 8min 2s\n",
    "- peak memory: 1780.96 MiB, increment: 1606.82 MiB, CPU times: user 7min 36s, sys:  39.1 s, total: 8min 15s, Wall time: 7min 21s\n",
    "\n",
    "\n",
    "In general, when we run the task of combining the data, computer random access memory (RAM) consumes very much. As you can see above, our computers took more than 8 minutes to run, and peak memories were all around 1000 MiB, but one of our team members' peak memories was around 3000 MiB (He uses Linux; everyone else in our team uses MAC System). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-stationery",
   "metadata": {},
   "source": [
    "# 3. EDA with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-electronics",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smaller-error",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 165.20 MiB, increment: 22.82 MiB\n",
      "1.09 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "combined_data = dd.read_csv(\"../data/combined_data.csv/*\")\n",
    "combined_data = combined_data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attempted-brunei",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)',\n",
      "       'model'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-presence",
   "metadata": {},
   "source": [
    "### Method 1: Using Dask only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-relative",
   "metadata": {},
   "source": [
    "Find geographical range of all records and mean rainfall level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "middle-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lat_min is -36.4674\n",
      "Maximum lat_max is -27.9061\n",
      "Minimum lon_min is 140.6250\n",
      "Maximum lon_max is 155.6250\n",
      "Mean rainfall is 1.9018\n",
      "peak memory: 1403.36 MiB, increment: 1250.46 MiB\n",
      "8min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "print(f\"Minimum lat_min is %0.4f\" % \n",
    "      combined_data['lat_min'].astype('float64').min().compute())\n",
    "print(f\"Maximum lat_max is %0.4f\" % \n",
    "      combined_data['lat_max'].astype('float64').max().compute())\n",
    "print(f\"Minimum lon_min is %0.4f\" % \n",
    "      combined_data['lon_min'].astype('float64').min().compute())\n",
    "print(f\"Maximum lon_max is %0.4f\" % \n",
    "      combined_data['lon_max'].astype('float64').max().compute())\n",
    "print(f\"Mean rainfall is %0.4f\" % \n",
    "      combined_data['rain (mm/day)'].astype('float64').mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-lobby",
   "metadata": {},
   "source": [
    "### Method 2: Using Dask and change datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-november",
   "metadata": {},
   "source": [
    "#### Default datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "southwest-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time             object\n",
      "lat_min          object\n",
      "lat_max          object\n",
      "lon_min          object\n",
      "lon_max          object\n",
      "rain (mm/day)    object\n",
      "model            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-balloon",
   "metadata": {},
   "source": [
    "#### Changing to `float32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "choice-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with 'object' dtype:\n",
      "Index                22016\n",
      "lat_max          500110904\n",
      "lat_min          500110904\n",
      "lon_max          500110904\n",
      "lon_min          500110904\n",
      "rain (mm/day)    500110904\n",
      "dtype: int64\n",
      "\n",
      "Memory usage after changing to float32:\n",
      "Index                22016\n",
      "lat_max          250055452\n",
      "lat_min          250055452\n",
      "lon_max          250055452\n",
      "lon_min          250055452\n",
      "rain (mm/day)    250055452\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = ['lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)']\n",
    "\n",
    "print(\"Memory usage with 'object' dtype:\")\n",
    "print(combined_data[numeric_columns].memory_usage().compute())\n",
    "\n",
    "# Change dtype\n",
    "combined_data[numeric_columns] = combined_data[numeric_columns].astype('float32')\n",
    "    \n",
    "print(\"\\nMemory usage after changing to float32:\")\n",
    "print(combined_data[numeric_columns].memory_usage().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-guitar",
   "metadata": {},
   "source": [
    "#### Perform the same EDA with `float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-texas",
   "metadata": {},
   "source": [
    "Find geographical range of all records and mean rainfall level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "round-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum lat_min is -36.4674\n",
      "Maximum lat_max is -27.9061\n",
      "Minimum lon_min is 140.6250\n",
      "Maximum lon_max is 155.6250\n",
      "Mean rainfall is 1.9018\n",
      "peak memory: 1559.61 MiB, increment: 1429.14 MiB\n",
      "8min 55s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "%%memit\n",
    "\n",
    "print(f\"Minimum lat_min is %0.4f\" % \n",
    "      combined_data['lat_min'].min().compute())\n",
    "print(f\"Maximum lat_max is %0.4f\" % \n",
    "      combined_data['lat_max'].max().compute())\n",
    "print(f\"Minimum lon_min is %0.4f\" % \n",
    "      combined_data['lon_min'].min().compute())\n",
    "print(f\"Maximum lon_max is %0.4f\" % \n",
    "      combined_data['lon_max'].max().compute())\n",
    "print(f\"Mean rainfall is %0.4f\" % \n",
    "      combined_data['rain (mm/day)'].mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-grounds",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Loading:\n",
    "- Since the `combined_data` was stored into partitions with Dask, we had to load them with Dask also. Dask takes care of loading in chunks and in parallel. \n",
    "\n",
    "EDA with Dask:\n",
    "- Using Dask to perform the EDA, the peak memory usage was around 1403MB, which isn't that high due to Dask's partitioning and parallel mechanism. It took around 8 minutes to process all 5 columns. \n",
    "\n",
    "EDA with Dask and change dtype:\n",
    "- Originally, Dask saved all columns as `object` datatype. After changing the numeric columns to `float32`, the memory usage went from around 500MB per column to around 250MB per column. <br> \n",
    "- Performing the same EDA, the peak memory usage is around the same level ~1500MB. This is because `%memit` measures the memory usage of the operation, instead of the data size. So although the data size decreased, the memory stack occupied by this code cell didn't change much. The fluctuation was probably due to other programs in the background affecting the memory I/O. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-mount",
   "metadata": {},
   "source": [
    "# 4. EDA with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radical-narrative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 9008.76 MiB, increment: 8493.55 MiB\n",
      "CPU times: user 2min 42s, sys: 2min 28s, total: 5min 11s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "combined_data.compute().reset_index().to_feather('../data/combined_data_feather.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tired-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5G\t../data/combined_data.csv\n",
      "1.4G\t../data/combined_data_feather.feather\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/combined_data_feather.feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-public",
   "metadata": {},
   "source": [
    "- We can see the file size is much smaller in a feather file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-satellite",
   "metadata": {},
   "source": [
    "## Setting up R environment + importing feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "western-sender",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appointed-dictionary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(arrow)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "herbal-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "\u001b[90m# A tibble: 92,040 x 2\u001b[39m\n",
      "   time                    n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m               \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m 1889-01-01             29\n",
      "\u001b[90m 2\u001b[39m 1889-01-01 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 3\u001b[39m 1889-01-02             29\n",
      "\u001b[90m 4\u001b[39m 1889-01-02 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 5\u001b[39m 1889-01-03             29\n",
      "\u001b[90m 6\u001b[39m 1889-01-03 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 7\u001b[39m 1889-01-04             29\n",
      "\u001b[90m 8\u001b[39m 1889-01-04 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m 9\u001b[39m 1889-01-05             29\n",
      "\u001b[90m10\u001b[39m 1889-01-05 12:00:00  \u001b[4m1\u001b[24m330\n",
      "\u001b[90m# … with 92,030 more rows\u001b[39m\n",
      "Time difference of 23.84748 secs\n",
      "CPU times: user 21.5 s, sys: 10.8 s, total: 32.2 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "\n",
    "combined_data_r <- arrow::read_feather(\"../data/combined_data_feather.feather\")\n",
    "print(class(combined_data_r))\n",
    "result <- combined_data_r %>% count(time)\n",
    "\n",
    "end_time <- Sys.time()\n",
    "print(result)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-pixel",
   "metadata": {},
   "source": [
    "## perform the EDA with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "urban-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of rows:62513863\"\n",
      "[1] \"Number of cols:8\"\n",
      "[1] \"Column names are...\"\n",
      "[1] \"index\"         \"time\"          \"lat_min\"       \"lat_max\"      \n",
      "[5] \"lon_min\"       \"lon_max\"       \"rain (mm/day)\" \"model\"        \n",
      "CPU times: user 20.5 ms, sys: 9.43 ms, total: 30 ms\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "print(paste0(\"Number of rows:\", nrow(combined_data_r)))\n",
    "print(paste0(\"Number of cols:\", ncol(combined_data_r)))\n",
    "print(\"Column names are...\")\n",
    "print(paste0(colnames(combined_data_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "egyptian-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 8\u001b[39m\n",
      "  index time          lat_min lat_max lon_min lon_max `rain (mm/day)` model     \n",
      "  \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m     \n",
      "\u001b[90m1\u001b[39m     0 1889-01-01 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.24\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m2\u001b[39m     1 1889-01-02 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.22\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m3\u001b[39m     2 1889-01-03 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.50\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m4\u001b[39m     3 1889-01-04 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.25\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m5\u001b[39m     4 1889-01-05 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.27\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "\u001b[90m6\u001b[39m     5 1889-01-06 1…   -\u001b[31m35\u001b[39m\u001b[31m.\u001b[39m\u001b[31m4\u001b[39m   -\u001b[31m33\u001b[39m\u001b[31m.\u001b[39m\u001b[31m6\u001b[39m    142.    143.        4.20\u001b[90me\u001b[39m\u001b[31m-13\u001b[39m MPI-ESM-1…\n",
      "CPU times: user 49.2 ms, sys: 6.95 ms, total: 56.2 ms\n",
      "Wall time: 54.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "head(combined_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cooked-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 6 x 8\u001b[39m\n",
      "   index time           lat_min lat_max lon_min lon_max `rain (mm/day)` model   \n",
      "   \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m            \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m           \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \n",
      "\u001b[90m1\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m958 2014-12-26 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            4.44 SAM0-UN…\n",
      "\u001b[90m2\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m959 2014-12-27 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            6.69 SAM0-UN…\n",
      "\u001b[90m3\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m960 2014-12-28 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            7.86 SAM0-UN…\n",
      "\u001b[90m4\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m961 2014-12-29 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.           10.0  SAM0-UN…\n",
      "\u001b[90m5\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m962 2014-12-30 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.            8.54 SAM0-UN…\n",
      "\u001b[90m6\u001b[39m \u001b[4m1\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m963 2014-12-31 12…   -\u001b[31m30\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m   -\u001b[31m29\u001b[39m\u001b[31m.\u001b[39m\u001b[31m2\u001b[39m    153.    154.           68.1  SAM0-UN…\n",
      "CPU times: user 40.5 ms, sys: 6.01 ms, total: 46.5 ms\n",
      "Wall time: 44.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "tail(combined_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "found-minutes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"lat_min Minimum: -36.46738961176\"\n",
      "[1] \"lat_min Maximum: -29.9\"\n",
      "[1] \"lat_min Mean: -33.1048166975557\"\n",
      "[1] \"lat_max Minimum: -36\"\n",
      "[1] \"lat_max Maximum: -27.9060644734869\"\n",
      "[1] \"lat_max Mean: -31.9775662186059\"\n",
      "[1] \"Rainfall Mean: 1.90182700665884\"\n",
      "CPU times: user 2.47 s, sys: 1.3 s, total: 3.77 s\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "print(paste0(\"lat_min Minimum: \", min(combined_data_r$lat_min, na.rm=T)))\n",
    "print(paste0(\"lat_min Maximum: \", max(combined_data_r$lat_min, na.rm=T)))\n",
    "print(paste0(\"lat_min Mean: \", mean(combined_data_r$lat_min, na.rm=T)))\n",
    "print(paste0(\"lat_max Minimum: \", min(combined_data_r$lat_max, na.rm=T)))\n",
    "print(paste0(\"lat_max Maximum: \", max(combined_data_r$lat_max, na.rm=T)))\n",
    "print(paste0(\"lat_max Mean: \", mean(combined_data_r$lat_max, na.rm=T)))\n",
    "print(paste0(\"Rainfall Mean: \", mean(combined_data_r$'rain (mm/day)', na.rm=T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-globe",
   "metadata": {},
   "source": [
    "## Why we used a .feather file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-fancy",
   "metadata": {},
   "source": [
    "A .feather file was used mainly due to its lightweight/portable nature.\n",
    "- The format was quick for saving the data (less than 5 minutes to save the entire dataset) and loading the data (less than 24 seconds).\n",
    "- The format was to push a very large dataframe into a significantly smaller file size than what was seen in our CSV file (1.4G vs 6.5G). \n",
    "- The format is language agnostic so we will be able to read the dataframe into either python.\n",
    "- The format was something that we were familiar with from previous projects.\n",
    "\n",
    "We chose feather instead of Parquet because...\n",
    "- Technically a Parquet file could have compressed the data further but it would have been more computationally expensive and thus slower. 1.4G is a very reasonable size for a dataset this big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-premium",
   "metadata": {},
   "source": [
    "# 5. Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-temple",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
